{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Nyoka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "#import PMML43Ext as pml\n",
    "import sys\n",
    "import nyoka.PMML43Ext as pml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 1\n",
    "config1 = {'robo1': 25, 'robo2': 26}\n",
    "\n",
    "def script1():\n",
    "    r3 = r1+r2\n",
    "    \n",
    "chunk1 = {\"config\": config1, 'for_preprocessing': script1, 'for_scoring': script1}\n",
    "\n",
    "# Chunk 2\n",
    "config2 = {'robo1': 6666, 'robo2': 5555}\n",
    "\n",
    "def script2():\n",
    "    r6 = r1+r2+r3-r4\n",
    "    \n",
    "chunk2 = {\"config\": config2, 'for_preprocessing': script2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skl_to_pmml(*arg):\n",
    "    ll = len(arg)\n",
    "    src = []\n",
    "    for leng in range(ll):\n",
    "        p1 = pml.script(content=arg[leng]['config'], for_=\"Config\"+str(leng))\n",
    "        p1.anyAttributes_ = {\"type\": \"application/x-python\"}\n",
    "        p1.export(sys.stdout,1)\n",
    "        p2 = pml.script(content=inspect.getsource(arg[leng]['for_preprocessing']), for_=\"for_preprocessing\")\n",
    "        p2.anyAttributes_ = {\"model\": \"LGBM\"}\n",
    "        p2.export(sys.stdout,1)\n",
    "        p3 = pml.script(content=inspect.getsource(arg[leng]['for_preprocessing']), for_=\"for_scoring\")\n",
    "        p3.anyAttributes_ = {\"model\": \"XGB\"}\n",
    "        p3.export(sys.stdout,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_to_pmml(chunk1,chunk2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nyoka with Multiple Models - Installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T10:45:49.041567Z",
     "start_time": "2019-07-02T10:45:48.785814Z"
    }
   },
   "outputs": [],
   "source": [
    "import nyoka\n",
    "from nyoka import model_to_pmml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T10:45:50.456970Z",
     "start_time": "2019-07-02T10:45:49.272148Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('mapping', DataFrameMapper(default=False, df_out=False,\n",
       "        features=[(['sepal length (cm)', 'sepal width (cm)'], StandardScaler(copy=True, with_mean=True, with_std=True)), (['petal length (cm)', 'petal width (cm)'], Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0))],\n",
       "        input_df=False, sparse=False))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "irisd = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "irisd['Species'] = iris.target\n",
    "\n",
    "features = irisd.columns.drop('Species')\n",
    "target = 'Species'\n",
    "\n",
    "pipeline_obj1 = Pipeline([\n",
    "    (\"mapping\", DataFrameMapper([\n",
    "    (['sepal length (cm)', 'sepal width (cm)'], StandardScaler()) , \n",
    "    (['petal length (cm)', 'petal width (cm)'], Imputer())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "pipeline_obj1.fit(irisd[features])\n",
    "# pipeline_obj1.transform(irisd[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T10:45:50.486250Z",
     "start_time": "2019-07-02T10:45:50.459898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rff = RandomForestClassifier()\n",
    "rff.fit(irisd[features], irisd[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T10:45:51.192427Z",
     "start_time": "2019-07-02T10:45:51.153356Z"
    }
   },
   "outputs": [],
   "source": [
    "toExportDict={\n",
    "    'model1':{\n",
    "        'hyperparameters':None,\n",
    "        'preProcessingScript':None,\n",
    "        'pipelineObj':None,\n",
    "        'modelObj':rff,\n",
    "        'featuresUsed':features,\n",
    "        'targetName':target,\n",
    "        'postProcessingScript':None,\n",
    "        'taskType': 'trainAndscore'\n",
    "    }\n",
    "}\n",
    "\n",
    "pmml = model_to_pmml(toExportDict, pmml_f_name=\"chirag.pmml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "irisd = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "irisd['Species'] = iris.target\n",
    "\n",
    "features = irisd.columns.drop('Species')\n",
    "target = 'Species'\n",
    "\n",
    "pipeline_obj2 = Pipeline([\n",
    "    (\"mapping\", DataFrameMapper([\n",
    "    (['sepal length (cm)', 'sepal width (cm)'], StandardScaler()) , \n",
    "    (['petal length (cm)', 'petal width (cm)'], Imputer())\n",
    "    ])),\n",
    "    (\"rfc\", RandomForestClassifier(n_estimators = 100))\n",
    "])\n",
    "\n",
    "pipeline_obj2.fit(irisd[features], irisd[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toExportDict={\n",
    "    'model1':{\n",
    "        'hyperParameters':'',\n",
    "        'preProcessingScript':{'scripts':[script1,script2], 'scriptpurpose':['train','score']},\n",
    "        'pipelineObj':pipeline_obj1,\n",
    "        'postProcessingScript':{'scripts':[script1], 'scriptpurpose':['postprocess']},\n",
    "        'taskType': 'trainAndscore'\n",
    "    },\n",
    "    'model2':{\n",
    "        'hyperParameters':'',\n",
    "        'preProcessingScript':{'scripts':[script1,script2], 'scriptpurpose':['train','score']},\n",
    "        'pipelineObj':pipeline_obj2,\n",
    "        'postProcessingScript':{'scripts':[script1], 'scriptpurpose':['postprocess']},\n",
    "        'taskType': 'score'\n",
    "    }   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_to_pmml(toExportDict, features, target, \"multiple_models_pmml.pmml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple MiningBuldingTask in Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entire_string(pipe0):\n",
    "    pipe_steps = pipe0.steps\n",
    "    pipe_memory = 'memory=' + str(pipe0.memory)\n",
    "    df_container = ''\n",
    "    pipe_container = ''\n",
    "    for step_idx, step in enumerate(pipe_steps):\n",
    "        pipe_step_container = ''\n",
    "        step_name = step[0]\n",
    "        step_item = step[1]\n",
    "        if step_item.__class__.__name__ == \"DataFrameMapper\":\n",
    "            df_default_val = \"default=\" + str(step_item.default)\n",
    "            df_out_val = \"df_out=\" + str(step_item.df_out)\n",
    "            input_df_val = \"input_df=\" + str(step_item.input_df)\n",
    "            sparse_val = \"sparse=\" + str(step_item.sparse)\n",
    "            for feature in step_item.features:\n",
    "                if not df_container:\n",
    "                    df_container = df_container + str(feature)\n",
    "                else:\n",
    "                    df_container = df_container + ',' + str(feature)\n",
    "            df_container = '[' + df_container + ']'\n",
    "            df_container = 'features=' + df_container\n",
    "            df_container = df_default_val + ',' + df_out_val + ',\\n\\t' + df_container\n",
    "            df_container = df_container + ',\\n\\t' + input_df_val + ',' + sparse_val\n",
    "            df_container = '(' + df_container + ')'\n",
    "            df_container = 'DataFrameMapper' + df_container\n",
    "            df_container = '\\'' + step_name + '\\'' + ',' + df_container\n",
    "            df_container = '(' + df_container + ')'\n",
    "        else:\n",
    "            pipe_step_container = '\\'' + step_name + '\\'' + ',' + str(step_item)\n",
    "            pipe_step_container = '(' + pipe_step_container + ')'\n",
    "            if not pipe_container:\n",
    "                pipe_container = pipe_container + pipe_step_container\n",
    "            else:\n",
    "                pipe_container = pipe_container + ',' + pipe_step_container\n",
    "    if df_container:\n",
    "        pipe_container = df_container + ',' + pipe_container\n",
    "    pipe_container = '[' + pipe_container + ']'\n",
    "    pipe_container = 'steps=' + pipe_container\n",
    "    pipe_container = pipe_memory + ',\\n    ' + pipe_container\n",
    "    pipe_container = 'Pipeline(' + pipe_container + ')'\n",
    "\n",
    "    return pipe_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mining_buildtask(toExportDict):\n",
    "    extension = []\n",
    "    for model_name in toExportDict.keys():\n",
    "        pipeline = toExportDict[model_name]['pipelineObj']\n",
    "        pipeline = get_entire_string(pipeline)\n",
    "        extension.append(pml.Extension(value=pipeline,name=model_name))\n",
    "    mining_bld_task = pml.MiningBuildTask(Extension = extension)\n",
    "    return mining_bld_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmml = pml.PMML(\n",
    "    #version=get_version(),\n",
    "    #Header=get_header(),\n",
    "    MiningBuildTask=get_mining_buildtask(toExportDict),\n",
    "    #DataDictionary=get_data_dictionary(model, col_names, target_name, categoric_values),\n",
    "    #**trfm_dict_kwargs,\n",
    "    #**models_dict\n",
    ")\n",
    "pmml.export(sys.stdout,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script tags implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toExportDict={\n",
    "    'model1':{\n",
    "        'hyperParameters':'',\n",
    "        'preProcessingScript':{'scripts':[script1,script2], 'scriptpurpose':['train','score']},\n",
    "        'pipelineObj':pipeline_obj1,\n",
    "        'postProcessingScript':{'scripts':[script1], 'scriptpurpose':['postprocess']},\n",
    "    },\n",
    "    'model2':{\n",
    "        'hyperParameters':'',\n",
    "        'preProcessingScript':{'scripts':[script1,script2], 'scriptpurpose':['train','score']},\n",
    "        'pipelineObj':pipeline_obj2,\n",
    "        'postProcessingScript':{'scripts':[script1], 'scriptpurpose':['postprocess']},\n",
    "    }   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrps = []\n",
    "for model_name in toExportDict.keys():\n",
    "    if toExportDict[model_name]['preProcessingScript'] is not None:\n",
    "        lstlen = len(toExportDict[model_name]['preProcessingScript']['scripts'])\n",
    "        for leng in range(lstlen):\n",
    "            scrps.append(pml.script(content=inspect.getsource(toExportDict[model_name]['preProcessingScript']['scripts'][leng]), \n",
    "                                    for_= model_name, \n",
    "                                    class_ = 'preprocessing',\n",
    "                                    scriptPurpose = toExportDict[model_name]['preProcessingScript']['scriptpurpose'][leng]\n",
    "                                    ))\n",
    "    if toExportDict[model_name]['postProcessingScript'] is not None:\n",
    "        lstlen = len(toExportDict[model_name]['postProcessingScript']['scripts'])\n",
    "        for leng in range(0,lstlen):\n",
    "            scrps.append(pml.script(content=inspect.getsource(toExportDict[model_name]['postProcessingScript']['scripts'][leng]), \n",
    "                                    for_= model_name, \n",
    "                                    class_ = 'postprocessing',\n",
    "                                    scriptPurpose = toExportDict[model_name]['postProcessingScript']['scriptpurpose'][leng]\n",
    "                                   ))\n",
    "        \n",
    "pmml = pml.PMML(\n",
    "    #version=get_version(),\n",
    "    #Header=get_header(),\n",
    "    #MiningBuildTask=get_mining_buildtask(toExportDict),\n",
    "    #DataDictionary=get_data_dictionary(model, col_names, target_name, categoric_values),\n",
    "    script = scrps,\n",
    "    #**trfm_dict_kwargs,\n",
    "    #**models_dict\n",
    ")\n",
    "pmml.export(sys.stdout,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1) taskType only for RegressionModel?\n",
    "\n",
    "2) Where will we have the information of what script will be used for pre-processings and which one for post-processing in PMML?\n",
    "\n",
    "3) How should we pass the information of combining 3 model predictions (by mean etc.) in the PMML?\n",
    "\n",
    "4) What if we have scenario like this (2 sklearn models for training, one Keras model for scoring). We have sklearn and Keras exporters separate :(\n",
    "\n",
    "5) scriptPurpose - is it already supported?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
